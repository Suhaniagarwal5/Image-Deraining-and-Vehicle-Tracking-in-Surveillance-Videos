{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VndPsOrGvDow"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# Install required libraries\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install numpy opencv-python tqdm scikit-image\n",
        "\n",
        "# Verify GPU availability\n",
        "print(f\"GPU is available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Create project directory\n",
        "project_dir = \"/content/Restormer_Project\"\n",
        "os.makedirs(project_dir, exist_ok=True)\n",
        "print(f\"Project directory created at {project_dir}\")\n",
        "\n",
        "# Set device for PyTorch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8-KwoAKv4S3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define paths\n",
        "project_dir = \"/content/Restormer_Project\"\n",
        "dataset_dir = os.path.join(project_dir, \"Rain100H\")\n",
        "\n",
        "# Upload and extract Rain100H dataset\n",
        "print(\"Please upload the Rain100H ZIP file from your laptop...\")\n",
        "uploaded = files.upload()\n",
        "zip_path = list(uploaded.keys())[0]\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(dataset_dir)\n",
        "print(f\"Extracted Rain100H to {dataset_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjT6efj0yu6B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Define the dataset directory\n",
        "dataset_dir = \"/content/Restormer_Project/Rain100H\"\n",
        "\n",
        "# Function to recursively list all files and folders\n",
        "def list_files(startpath):\n",
        "    print(f\"Exploring directory: {startpath}\")\n",
        "    for root, dirs, files in os.walk(startpath):\n",
        "        level = root.replace(startpath, '').count(os.sep)\n",
        "        indent = ' ' * 4 * level\n",
        "        print(f\"{indent}{os.path.basename(root)}/\")\n",
        "        for f in sorted(files):\n",
        "            print(f\"{indent}    {f}\")\n",
        "\n",
        "# List the contents of the dataset directory\n",
        "list_files(dataset_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuU6UBgKzbHc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# Define paths\n",
        "dataset_dir = \"/content/Restormer_Project/Rain100H\"\n",
        "\n",
        "# Define dataset class\n",
        "class Rain100HDataset(Dataset):\n",
        "    def __init__(self, root_dir, file_indices, transform=None, augment=False):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.augment = augment\n",
        "        # Get all rain and norain files\n",
        "        all_rain_files = sorted([f for f in os.listdir(root_dir) if f.startswith('rain-') and f.endswith('.png')])\n",
        "        all_norain_files = sorted([f for f in os.listdir(root_dir) if f.startswith('norain-') and f.endswith('.png')])\n",
        "        assert len(all_rain_files) == len(all_norain_files), \"Mismatched number of rainy and clean images\"\n",
        "        # Use specified indices\n",
        "        self.rain_files = [all_rain_files[i] for i in file_indices]\n",
        "        self.norain_files = [all_norain_files[i] for i in file_indices]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rain_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        rain_path = os.path.join(self.root_dir, self.rain_files[idx])\n",
        "        norain_path = os.path.join(self.root_dir, self.norain_files[idx])\n",
        "        rain_img = cv2.imread(rain_path, cv2.IMREAD_COLOR)\n",
        "        norain_img = cv2.imread(norain_path, cv2.IMREAD_COLOR)\n",
        "\n",
        "        if rain_img is None or norain_img is None:\n",
        "            raise ValueError(f\"Failed to load images: {rain_path} or {norain_path}\")\n",
        "\n",
        "        rain_img = cv2.cvtColor(rain_img, cv2.COLOR_BGR2RGB)\n",
        "        norain_img = cv2.cvtColor(norain_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Apply augmentations (random flips for training)\n",
        "        if self.augment:\n",
        "            if random.random() > 0.5:\n",
        "                rain_img = cv2.flip(rain_img, 1)  # Horizontal flip\n",
        "                norain_img = cv2.flip(norain_img, 1)\n",
        "            if random.random() > 0.5:\n",
        "                rain_img = cv2.flip(rain_img, 0)  # Vertical flip\n",
        "                norain_img = cv2.flip(norain_img, 0)\n",
        "\n",
        "        if self.transform:\n",
        "            rain_img = self.transform(rain_img)\n",
        "            norain_img = self.transform(norain_img)\n",
        "\n",
        "        return rain_img, norain_img\n",
        "\n",
        "# Define transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Converts to tensor and scales to [0,1]\n",
        "])\n",
        "\n",
        "# Create train/test split (80/20)\n",
        "num_images = 100  # Rain100H has 100 pairs\n",
        "indices = list(range(num_images))\n",
        "random.seed(42)  # For reproducibility\n",
        "random.shuffle(indices)\n",
        "train_indices = indices[:80]\n",
        "test_indices = indices[80:]\n",
        "\n",
        "# Create dataset instances\n",
        "train_dataset = Rain100HDataset(dataset_dir, train_indices, transform=transform, augment=True)\n",
        "test_dataset = Rain100HDataset(dataset_dir, test_indices, transform=transform, augment=False)\n",
        "print(f\"Found {len(train_dataset)} training pairs\")\n",
        "print(f\"Found {len(test_dataset)} test pairs\")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "# Visualize a sample image pair\n",
        "if len(train_dataset) > 0:\n",
        "    rain_img, norain_img = train_dataset[0]\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.title(\"Rainy Image\")\n",
        "    plt.imshow(rain_img.permute(1, 2, 0))\n",
        "    plt.axis('off')\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.title(\"Clean Image\")\n",
        "    plt.imshow(norain_img.permute(1, 2, 0))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No images loaded. Please check the dataset structure.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbBUZ3222yIm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Multi-Dconv Head Transposed Attention (MDTA)\n",
        "class MDTA(nn.Module):\n",
        "    def __init__(self, dim, num_heads):\n",
        "        super(MDTA, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
        "\n",
        "        self.qkv = nn.Conv2d(dim, dim * 3, kernel_size=1, bias=False)\n",
        "        self.qkv_dwconv = nn.Conv2d(dim * 3, dim * 3, kernel_size=3, stride=1, padding=1, groups=dim * 3, bias=False)\n",
        "        self.project_out = nn.Conv2d(dim, dim, kernel_size=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        x = self.qkv(x)\n",
        "        x = self.qkv_dwconv(x)\n",
        "        qkv = x.chunk(3, dim=1)\n",
        "        q, k, v = [x.reshape(b, self.num_heads, -1, h * w) for x in qkv]\n",
        "\n",
        "        q = q.transpose(-2, -1)  # (b, num_heads, h*w, c//num_heads)\n",
        "        k = k  # (b, num_heads, c//num_heads, h*w)\n",
        "        v = v  # (b, num_heads, c//num_heads, h*w)\n",
        "\n",
        "        attn = (k @ q) * self.temperature  # (b, num_heads, c//num_heads, c//num_heads)\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        out = (attn @ v)  # (b, num_heads, c//num_heads, h*w)\n",
        "        out = out.transpose(-2, -1).reshape(b, c, h, w)\n",
        "        out = self.project_out(out)\n",
        "        return out\n",
        "\n",
        "# Gated-Dconv Feed-Forward Network (GDFN)\n",
        "class GDFN(nn.Module):\n",
        "    def __init__(self, dim, ffn_expansion_factor=2.66):\n",
        "        super(GDFN, self).__init__()\n",
        "        hidden_features = int(dim * ffn_expansion_factor)\n",
        "\n",
        "        self.project_in = nn.Conv2d(dim, hidden_features * 2, kernel_size=1, bias=False)\n",
        "        self.dwconv = nn.Conv2d(hidden_features * 2, hidden_features * 2, kernel_size=3, stride=1, padding=1, groups=hidden_features * 2, bias=False)\n",
        "        self.project_out = nn.Conv2d(hidden_features, dim, kernel_size=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.project_in(x)\n",
        "        x1, x2 = self.dwconv(x).chunk(2, dim=1)\n",
        "        x = F.gelu(x1) * x2\n",
        "        x = self.project_out(x)\n",
        "        return x\n",
        "\n",
        "# Transformer Block\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = MDTA(dim, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.ffn = GDFN(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        x = x + self.attn(self.norm1(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2))\n",
        "        x = x + self.ffn(self.norm2(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2))\n",
        "        return x\n",
        "\n",
        "# Restormer Model\n",
        "class Restormer(nn.Module):\n",
        "    def __init__(self, inp_channels=3, out_channels=3, dim=48, num_blocks=[2, 3, 3, 4], num_refinement_blocks=2, heads=[1, 2, 4, 8]):\n",
        "        super(Restormer, self).__init__()\n",
        "        self.patch_embed = nn.Conv2d(inp_channels, dim, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder_level1 = nn.Sequential(*[TransformerBlock(dim=dim, num_heads=heads[0]) for _ in range(num_blocks[0])])\n",
        "        self.down1_2 = nn.Conv2d(dim, dim*2, kernel_size=2, stride=2)\n",
        "        self.encoder_level2 = nn.Sequential(*[TransformerBlock(dim=dim*2, num_heads=heads[1]) for _ in range(num_blocks[1])])\n",
        "        self.down2_3 = nn.Conv2d(dim*2, dim*4, kernel_size=2, stride=2)\n",
        "        self.encoder_level3 = nn.Sequential(*[TransformerBlock(dim=dim*4, num_heads=heads[2]) for _ in range(num_blocks[2])])\n",
        "        self.down3_4 = nn.Conv2d(dim*4, dim*8, kernel_size=2, stride=2)\n",
        "        self.encoder_level4 = nn.Sequential(*[TransformerBlock(dim=dim*8, num_heads=heads[3]) for _ in range(num_blocks[3])])\n",
        "\n",
        "        # Decoder\n",
        "        self.up4_3 = nn.ConvTranspose2d(dim*8, dim*4, kernel_size=2, stride=2)\n",
        "        self.reduce_chan_level3 = nn.Conv2d(dim*8, dim*4, kernel_size=1)\n",
        "        self.decoder_level3 = nn.Sequential(*[TransformerBlock(dim=dim*4, num_heads=heads[2]) for _ in range(num_blocks[2])])\n",
        "\n",
        "        self.up3_2 = nn.ConvTranspose2d(dim*4, dim*2, kernel_size=2, stride=2)\n",
        "        self.reduce_chan_level2 = nn.Conv2d(dim*4, dim*2, kernel_size=1)\n",
        "        self.decoder_level2 = nn.Sequential(*[TransformerBlock(dim=dim*2, num_heads=heads[1]) for _ in range(num_blocks[1])])\n",
        "\n",
        "        self.up2_1 = nn.ConvTranspose2d(dim*2, dim, kernel_size=2, stride=2)\n",
        "        self.reduce_chan_level1 = nn.Conv2d(dim*2, dim, kernel_size=1) # Add this line\n",
        "        self.decoder_level1 = nn.Sequential(*[TransformerBlock(dim=dim, num_heads=heads[0]) for _ in range(num_blocks[0])])\n",
        "\n",
        "        # Refinement\n",
        "        self.refinement = nn.Sequential(*[TransformerBlock(dim=dim, num_heads=heads[0]) for _ in range(num_refinement_blocks)])\n",
        "        self.output = nn.Conv2d(dim, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "    def forward(self, inp_img):\n",
        "        # Encoder\n",
        "        inp_enc_level1 = self.patch_embed(inp_img)\n",
        "        print(f\"Encoder Level 1: {inp_enc_level1.shape}\")\n",
        "        out_enc_level1 = self.encoder_level1(inp_enc_level1)\n",
        "\n",
        "        inp_enc_level2 = self.down1_2(out_enc_level1)\n",
        "        print(f\"Encoder Level 2: {inp_enc_level2.shape}\")\n",
        "        out_enc_level2 = self.encoder_level2(inp_enc_level2)\n",
        "\n",
        "        inp_enc_level3 = self.down2_3(out_enc_level2)\n",
        "        print(f\"Encoder Level 3: {inp_enc_level3.shape}\")\n",
        "        out_enc_level3 = self.encoder_level3(inp_enc_level3)\n",
        "\n",
        "        inp_enc_level4 = self.down3_4(out_enc_level3)\n",
        "        print(f\"Encoder Level 4: {inp_enc_level4.shape}\")\n",
        "        out_enc_level4 = self.encoder_level4(inp_enc_level4)\n",
        "\n",
        "        # Decoder\n",
        "        inp_dec_level3 = self.up4_3(out_enc_level4)\n",
        "        inp_dec_level3 = torch.cat([inp_dec_level3, out_enc_level3], 1)\n",
        "        inp_dec_level3 = self.reduce_chan_level3(inp_dec_level3)\n",
        "        print(f\"Decoder Level 3: {inp_dec_level3.shape}\")\n",
        "        out_dec_level3 = self.decoder_level3(inp_dec_level3)\n",
        "\n",
        "        inp_dec_level2 = self.up3_2(out_dec_level3)\n",
        "        inp_dec_level2 = torch.cat([inp_dec_level2, out_enc_level2], 1)\n",
        "        inp_dec_level2 = self.reduce_chan_level2(inp_dec_level2)\n",
        "        print(f\"Decoder Level 2: {inp_dec_level2.shape}\")\n",
        "        out_dec_level2 = self.decoder_level2(inp_dec_level2)\n",
        "\n",
        "        inp_dec_level1 = self.up2_1(out_dec_level2)\n",
        "        inp_dec_level1 = torch.cat([inp_dec_level1, out_enc_level1], 1)\n",
        "        inp_dec_level1 = self.reduce_chan_level1(inp_dec_level1) # Add this line\n",
        "        print(f\"Decoder Level 1: {inp_dec_level1.shape}\")\n",
        "        out_dec_level1 = self.decoder_level1(inp_dec_level1)\n",
        "\n",
        "        # Refinement and output\n",
        "        out_dec_level1 = self.refinement(out_dec_level1)\n",
        "        print(f\"Refinement: {out_dec_level1.shape}\")\n",
        "        out_dec_level1 = self.output(out_dec_level1)\n",
        "\n",
        "        # Residual connection\n",
        "        return inp_img + out_dec_level1\n",
        "\n",
        "# Test the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Restormer().to(device)\n",
        "print(\"Model initialized successfully\")\n",
        "\n",
        "# Dummy input (batch_size=1, channels=3, height=128, width=128)\n",
        "dummy_input = torch.randn(1, 3, 128, 128).to(device)\n",
        "output = model(dummy_input)\n",
        "print(f\"Input shape: {dummy_input.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Model moved to {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7iGKnuu3Ojh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import cv2\n",
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# Assume Rain100HDataset from Section 2 is defined\n",
        "class Rain100HDataset(Dataset):\n",
        "    def __init__(self, root_dir, file_indices, transform=None, augment=False, patch_size=128):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.augment = augment\n",
        "        self.patch_size = patch_size\n",
        "        all_rain_files = sorted([f for f in os.listdir(root_dir) if f.startswith('rain-') and f.endswith('.png')])\n",
        "        all_norain_files = sorted([f for f in os.listdir(root_dir) if f.startswith('norain-') and f.endswith('.png')])\n",
        "        assert len(all_rain_files) == len(all_norain_files), \"Mismatched number of rainy and clean images\"\n",
        "        self.rain_files = [all_rain_files[i] for i in file_indices]\n",
        "        self.norain_files = [all_norain_files[i] for i in file_indices]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rain_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        rain_path = os.path.join(self.root_dir, self.rain_files[idx])\n",
        "        norain_path = os.path.join(self.root_dir, self.norain_files[idx])\n",
        "        rain_img = cv2.imread(rain_path, cv2.IMREAD_COLOR)\n",
        "        norain_img = cv2.imread(norain_path, cv2.IMREAD_COLOR)\n",
        "\n",
        "        if rain_img is None or norain_img is None:\n",
        "            raise ValueError(f\"Failed to load images: {rain_path} or {norain_path}\")\n",
        "\n",
        "        rain_img = cv2.cvtColor(rain_img, cv2.COLOR_BGR2RGB)\n",
        "        norain_img = cv2.cvtColor(norain_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Crop to patch_size\n",
        "        h, w = rain_img.shape[:2]\n",
        "        if h > self.patch_size and w > self.patch_size:\n",
        "            top = random.randint(0, h - self.patch_size)\n",
        "            left = random.randint(0, w - self.patch_size)\n",
        "            rain_img = rain_img[top:top+self.patch_size, left:left+self.patch_size]\n",
        "            norain_img = norain_img[top:top+self.patch_size, left:left+self.patch_size]\n",
        "\n",
        "        # Apply augmentations\n",
        "        if self.augment:\n",
        "            if random.random() > 0.5:\n",
        "                rain_img = cv2.flip(rain_img, 1)\n",
        "                norain_img = cv2.flip(norain_img, 1)\n",
        "            if random.random() > 0.5:\n",
        "                rain_img = cv2.flip(rain_img, 0)\n",
        "                norain_img = cv2.flip(norain_img, 0)\n",
        "\n",
        "        if self.transform:\n",
        "            rain_img = self.transform(rain_img)\n",
        "            norain_img = self.transform(norain_img)\n",
        "\n",
        "        return rain_img, norain_img\n",
        "\n",
        "# Assume Restormer model from Section 3 is defined\n",
        "# [Insert the Restormer, TransformerBlock, MDTA, GDFN classes from the previous section here]\n",
        "# For brevity, I'll assume they're already in the notebook. Copy them if needed.\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, criterion, optimizer, device, num_epochs, scaler):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, (rain_imgs, norain_imgs) in enumerate(train_loader):\n",
        "            rain_imgs, norain_imgs = rain_imgs.to(device), norain_imgs.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            with autocast():\n",
        "                outputs = model(rain_imgs)\n",
        "                loss = criterion(outputs, norain_imgs)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# Setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dataset_dir = \"/content/Restormer_Project/Rain100H\"\n",
        "num_images = 100\n",
        "indices = list(range(num_images))\n",
        "random.seed(42)\n",
        "random.shuffle(indices)\n",
        "train_indices = indices[:80]\n",
        "test_indices = indices[80:]\n",
        "\n",
        "# Progressive learning: train with increasing patch sizes\n",
        "patch_sizes = [128, 256]\n",
        "num_epochs_per_stage = 5\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "# Assume Restormer model is already defined in the notebook\n",
        "# model = Restormer().to(device)\n",
        "criterion = nn.L1Loss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-2)\n",
        "scaler = GradScaler()\n",
        "\n",
        "for patch_size in patch_sizes:\n",
        "    print(f\"Starting training with patch size {patch_size}x{patch_size}\")\n",
        "    train_dataset = Rain100HDataset(dataset_dir, train_indices, transform=transform, augment=True, patch_size=patch_size)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "    train_model(model, train_loader, criterion, optimizer, device, num_epochs_per_stage, scaler)\n",
        "\n",
        "# Save the model\n",
        "model_path = \"/content/Restormer_Project/restormer_rain100h.pth\"\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Model saved to {model_path}\")\n",
        "\n",
        "# Visualize a sample output\n",
        "model.eval()\n",
        "test_dataset = Rain100HDataset(dataset_dir, test_indices, transform=transform, augment=False, patch_size=256)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "rain_img, norain_img = next(iter(test_loader))\n",
        "rain_img, norain_img = rain_img.to(device), norain_img.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_img = model(rain_img)\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.title(\"Rainy Image\")\n",
        "plt.imshow(rain_img.cpu().squeeze().permute(1, 2, 0))\n",
        "plt.axis('off')\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.title(\"Ground Truth\")\n",
        "plt.imshow(norain_img.cpu().squeeze().permute(1, 2, 0))\n",
        "plt.axis('off')\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.title(\"Derained Output\")\n",
        "plt.imshow(output_img.cpu().squeeze().permute(1, 2, 0).clamp(0, 1))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iQDFN395UI1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import cv2\n",
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Rain100HDataset (updated with validation)\n",
        "class Rain100HDataset(Dataset):\n",
        "    def __init__(self, root_dir, file_indices, transform=None, augment=False, patch_size=128):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.augment = augment\n",
        "        self.patch_size = patch_size\n",
        "        all_rain_files = sorted([f for f in os.listdir(root_dir) if f.startswith('rain-') and f.endswith('.png')])\n",
        "        all_norain_files = sorted([f for f in os.listdir(root_dir) if f.startswith('norain-') and f.endswith('.png')])\n",
        "        assert len(all_rain_files) == len(all_norain_files), \"Mismatched number of rainy and clean images\"\n",
        "        self.rain_files = [all_rain_files[i] for i in file_indices]\n",
        "        self.norain_files = [all_norain_files[i] for i in file_indices]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rain_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        rain_path = os.path.join(self.root_dir, self.rain_files[idx])\n",
        "        norain_path = os.path.join(self.root_dir, self.norain_files[idx])\n",
        "        rain_img = cv2.imread(rain_path, cv2.IMREAD_COLOR)\n",
        "        norain_img = cv2.imread(norain_path, cv2.IMREAD_COLOR)\n",
        "\n",
        "        if rain_img is None or norain_img is None:\n",
        "            raise ValueError(f\"Failed to load images: {rain_path} or {norain_path}\")\n",
        "\n",
        "        rain_img = cv2.cvtColor(rain_img, cv2.COLOR_BGR2RGB)\n",
        "        norain_img = cv2.cvtColor(norain_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Crop to patch_size\n",
        "        h, w = rain_img.shape[:2]\n",
        "        if h > self.patch_size and w > self.patch_size:\n",
        "            top = random.randint(0, h - self.patch_size)\n",
        "            left = random.randint(0, w - self.patch_size)\n",
        "            rain_img = rain_img[top:top+self.patch_size, left:left+self.patch_size]\n",
        "            norain_img = norain_img[top:top+self.patch_size, left:left+self.patch_size]\n",
        "\n",
        "        # Apply augmentations\n",
        "        if self.augment:\n",
        "            if random.random() > 0.5:\n",
        "                rain_img = cv2.flip(rain_img, 1)\n",
        "                norain_img = cv2.flip(norain_img, 1)\n",
        "            if random.random() > 0.5:\n",
        "                rain_img = cv2.flip(rain_img, 0)\n",
        "                norain_img = cv2.flip(norain_img, 0)\n",
        "\n",
        "        if self.transform:\n",
        "            rain_img = self.transform(rain_img)\n",
        "            norain_img = self.transform(norain_img)\n",
        "\n",
        "        return rain_img, norain_img\n",
        "\n",
        "# Validate dataset\n",
        "def validate_dataset(root_dir, indices):\n",
        "    print(\"Validating dataset...\")\n",
        "    valid_indices = []\n",
        "    for idx in indices:\n",
        "        rain_path = os.path.join(root_dir, f\"rain-{idx+1:03d}.png\")\n",
        "        norain_path = os.path.join(root_dir, f\"norain-{idx+1:03d}.png\")\n",
        "        rain_img = cv2.imread(rain_path, cv2.IMREAD_COLOR)\n",
        "        norain_img = cv2.imread(norain_path, cv2.IMREAD_COLOR)\n",
        "        if rain_img is None or norain_img is None:\n",
        "            print(f\"Invalid image pair: {rain_path} or {norain_path}\")\n",
        "            continue\n",
        "        rain_img = cv2.cvtColor(rain_img, cv2.COLOR_BGR2RGB)\n",
        "        norain_img = cv2.cvtColor(norain_img, cv2.COLOR_BGR2RGB)\n",
        "        if np.any(np.isnan(rain_img)) or np.any(np.isinf(rain_img)) or \\\n",
        "           np.any(np.isnan(norain_img)) or np.any(np.isinf(norain_img)):\n",
        "            print(f\"Invalid pixel values in: {rain_path} or {norain_path}\")\n",
        "            continue\n",
        "        valid_indices.append(idx)\n",
        "    return valid_indices\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, criterion, optimizer, device, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, (rain_imgs, norain_imgs) in enumerate(train_loader):\n",
        "            rain_imgs, norain_imgs = rain_imgs.to(device), norain_imgs.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(rain_imgs)\n",
        "            loss = criterion(outputs, norain_imgs)\n",
        "\n",
        "            if torch.isnan(loss) or torch.isinf(loss):\n",
        "                print(f\"NaN/Inf loss detected at batch {i+1}, epoch {epoch+1}\")\n",
        "                continue\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Visualize sample output per epoch\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            sample_rain, sample_norain = next(iter(train_loader))\n",
        "            sample_rain, sample_norain = sample_rain[:1].to(device), sample_norain[:1].to(device)\n",
        "            sample_output = model(sample_rain)\n",
        "\n",
        "            plt.figure(figsize=(15, 5))\n",
        "            plt.subplot(1, 3, 1)\n",
        "            plt.title(\"Rainy Image\")\n",
        "            plt.imshow(sample_rain.cpu().squeeze().permute(1, 2, 0))\n",
        "            plt.axis('off')\n",
        "            plt.subplot(1, 3, 2)\n",
        "            plt.title(\"Ground Truth\")\n",
        "            plt.imshow(sample_norain.cpu().squeeze().permute(1, 2, 0))\n",
        "            plt.axis('off')\n",
        "            plt.subplot(1, 3, 3)\n",
        "            plt.title(f\"Derained (Epoch {epoch+1})\")\n",
        "            plt.imshow(sample_output.cpu().squeeze().permute(1, 2, 0).clamp(0, 1))\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "        model.train()\n",
        "\n",
        "# Setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dataset_dir = \"/content/Restormer_Project/Rain100H\"\n",
        "output_dir = os.path.join(dataset_dir, \"outputs\")\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Validate dataset\n",
        "num_images = 100\n",
        "indices = list(range(num_images))\n",
        "random.seed(42)\n",
        "random.shuffle(indices)\n",
        "train_indices = indices[:80]\n",
        "test_indices = indices[80:]\n",
        "train_indices = validate_dataset(dataset_dir, train_indices)\n",
        "test_indices = validate_dataset(dataset_dir, test_indices)\n",
        "print(f\"Found {len(train_indices)} valid training pairs, {len(test_indices)} valid test pairs\")\n",
        "\n",
        "# Progressive learning\n",
        "patch_sizes = [128, 192]\n",
        "num_epochs_per_stage = 5\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "model = Restormer().to(device)\n",
        "criterion = nn.L1Loss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
        "\n",
        "for patch_size in patch_sizes:\n",
        "    print(f\"Starting training with patch size {patch_size}x{patch_size}\")\n",
        "    train_dataset = Rain100HDataset(dataset_dir, train_indices, transform=transform, augment=True, patch_size=patch_size)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "    train_model(model, train_loader, criterion, optimizer, device, num_epochs_per_stage)\n",
        "\n",
        "    # Save model after each stage\n",
        "    model_path = f\"/content/Restormer_Project/restormer_rain100h_patch{patch_size}.pth\"\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f\"Model saved to {model_path}\")\n",
        "\n",
        "# Visualize final test output\n",
        "model.eval()\n",
        "test_dataset = Rain100HDataset(dataset_dir, test_indices, transform=transform, augment=False, patch_size=256)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "rain_img, norain_img = next(iter(test_loader))\n",
        "rain_img, norain_img = rain_img.to(device), norain_img.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_img = model(rain_img)\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.title(\"Rainy Image\")\n",
        "    plt.imshow(rain_img.cpu().squeeze().permute(1, 2, 0))\n",
        "    plt.axis('off')\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.title(\"Ground Truth\")\n",
        "    plt.imshow(norain_img.cpu().squeeze().permute(1, 2, 0))\n",
        "    plt.axis('off')\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.title(\"Final Derained Output\")\n",
        "    plt.imshow(output_img.cpu().squeeze().permute(1, 2, 0).clamp(0, 1))\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2cW12KF8bzl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import cv2\n",
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import pandas as pd\n",
        "\n",
        "# Rain100HDataset (from Section 2, updated for evaluation)\n",
        "class Rain100HDataset(Dataset):\n",
        "    def __init__(self, root_dir, file_indices, transform=None, patch_size=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.patch_size = patch_size\n",
        "        all_rain_files = sorted([f for f in os.listdir(root_dir) if f.startswith('rain-') and f.endswith('.png')])\n",
        "        all_norain_files = sorted([f for f in os.listdir(root_dir) if f.startswith('norain-') and f.endswith('.png')])\n",
        "        assert len(all_rain_files) == len(all_norain_files), \"Mismatched number of rainy and clean images\"\n",
        "        self.rain_files = [all_rain_files[i] for i in file_indices]\n",
        "        self.norain_files = [all_norain_files[i] for i in file_indices]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rain_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        rain_path = os.path.join(self.root_dir, self.rain_files[idx])\n",
        "        norain_path = os.path.join(self.root_dir, self.norain_files[idx])\n",
        "        rain_img = cv2.imread(rain_path, cv2.IMREAD_COLOR)\n",
        "        norain_img = cv2.imread(norain_path, cv2.IMREAD_COLOR)\n",
        "\n",
        "        if rain_img is None or norain_img is None:\n",
        "            raise ValueError(f\"Failed to load images: {rain_path} or {norain_path}\")\n",
        "\n",
        "        rain_img = cv2.cvtColor(rain_img, cv2.COLOR_BGR2RGB)\n",
        "        norain_img = cv2.cvtColor(norain_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Crop to patch_size if specified\n",
        "        if self.patch_size:\n",
        "            h, w = rain_img.shape[:2]\n",
        "            if h > self.patch_size and w > self.patch_size:\n",
        "                top = (h - self.patch_size) // 2  # Center crop for evaluation\n",
        "                left = (w - self.patch_size) // 2\n",
        "                rain_img = rain_img[top:top+self.patch_size, left:left+self.patch_size]\n",
        "                norain_img = norain_img[top:top+self.patch_size, left:left+self.patch_size]\n",
        "\n",
        "        if self.transform:\n",
        "            rain_img = self.transform(rain_img)\n",
        "            norain_img = self.transform(norain_img)\n",
        "\n",
        "        return rain_img, norain_img, self.rain_files[idx]\n",
        "\n",
        "# Assume Restormer, MDTA, GDFN, TransformerBlock classes are defined (from Section 3)\n",
        "# [Insert Restormer, MDTA, GDFN, TransformerBlock classes here if not already in notebook]\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    psnr_scores = []\n",
        "    ssim_scores = []\n",
        "    results = []\n",
        "\n",
        "    output_dir = \"/content/Restormer_Project/outputs/derained\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (rain_img, norain_img, filename_tuple) in enumerate(test_loader):\n",
        "            rain_img, norain_img = rain_img.to(device), norain_img.to(device)\n",
        "\n",
        "            output_img = model(rain_img)\n",
        "\n",
        "            # Convert to numpy for metrics\n",
        "            output_np = output_img.cpu().squeeze().permute(1, 2, 0).clamp(0, 1).numpy()\n",
        "            norain_np = norain_img.cpu().squeeze().permute(1, 2, 0).numpy()\n",
        "            rain_np = rain_img.cpu().squeeze().permute(1, 2, 0).numpy()\n",
        "\n",
        "            # Compute PSNR and SSIM\n",
        "            psnr_score = psnr(norain_np, output_np, data_range=1.0)\n",
        "            ssim_score = ssim(norain_np, output_np, channel_axis=2, data_range=1.0, win_size=3)\n",
        "            psnr_scores.append(psnr_score)\n",
        "            ssim_scores.append(ssim_score)\n",
        "\n",
        "            results.append({\n",
        "                \"Image\": filename_tuple[0],\n",
        "                \"PSNR\": psnr_score,\n",
        "                \"SSIM\": ssim_score\n",
        "            })\n",
        "\n",
        "            # Save derained image\n",
        "            filename = filename_tuple[0]\n",
        "            output_path = os.path.join(output_dir, f\"derained_{filename}\")\n",
        "            plt.imsave(output_path, output_np)\n",
        "\n",
        "    avg_psnr = np.mean(psnr_scores)\n",
        "    avg_ssim = np.mean(ssim_scores)\n",
        "    return results, avg_psnr, avg_ssim\n",
        "\n",
        "# Visualize sample results\n",
        "def visualize_results(model, test_loader, device, num_samples=5):\n",
        "    model.eval()\n",
        "    samples = list(iter(test_loader))[:num_samples]\n",
        "\n",
        "    plt.figure(figsize=(15, 5 * num_samples))\n",
        "    with torch.no_grad():\n",
        "        for i, (rain_img, norain_img, filename_tuple) in enumerate(samples):\n",
        "            rain_img, norain_img = rain_img.to(device), norain_img.to(device)\n",
        "            output_img = model(rain_img)\n",
        "\n",
        "            filename = filename_tuple[0]\n",
        "\n",
        "            plt.subplot(num_samples, 3, i*3 + 1)\n",
        "            plt.title(f\"Rainy: {filename}\")\n",
        "            plt.imshow(rain_img.cpu().squeeze().permute(1, 2, 0))\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.subplot(num_samples, 3, i*3 + 2)\n",
        "            plt.title(\"Ground Truth\")\n",
        "            plt.imshow(norain_img.cpu().squeeze().permute(1, 2, 0))\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.subplot(num_samples, 3, i*3 + 3)\n",
        "            plt.title(\"Derained\")\n",
        "            plt.imshow(output_img.cpu().squeeze().permute(1, 2, 0).clamp(0, 1))\n",
        "            plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dataset_dir = \"/content/Restormer_Project/Rain100H\"\n",
        "model_path = \"/content/Restormer_Project/restormer_rain100h_patch192.pth\"\n",
        "\n",
        "# Load test dataset\n",
        "num_images = 100\n",
        "indices = list(range(num_images))\n",
        "random.seed(42)\n",
        "random.shuffle(indices)\n",
        "test_indices = indices[80:]\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "test_dataset = Rain100HDataset(dataset_dir, test_indices, transform=transform, patch_size=256)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Load model\n",
        "model = Restormer().to(device)\n",
        "print(f\"Loading model from {model_path}\")\n",
        "if os.path.exists(model_path):\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
        "\n",
        "# Evaluate\n",
        "print(f\"Evaluating on {len(test_dataset)} test pairs...\")\n",
        "results, avg_psnr, avg_ssim = evaluate_model(model, test_loader, device)\n",
        "print(f\"Average PSNR: {avg_psnr:.2f} dB\")\n",
        "print(f\"Average SSIM: {avg_ssim:.3f}\")\n",
        "\n",
        "# Save results to CSV\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(\"/content/Restormer_Project/outputs/evaluation_results.csv\", index=False)\n",
        "print(f\"Results saved to /content/Restormer_Project/outputs/evaluation_results.csv\")\n",
        "\n",
        "# Visualize 5 sample results\n",
        "visualize_results(model, test_loader, device, num_samples=5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from skimage.metrics import structural_similarity as ssim  # Already imported\n",
        "import pandas as pd # Import pandas\n",
        "\n",
        "# New metric functions (add to notebook)\n",
        "def calculate_mae(pred, gt):\n",
        "    return np.mean(np.abs(pred - gt))\n",
        "\n",
        "def calculate_nmse(pred, gt):\n",
        "    mse = np.mean((pred - gt) ** 2)\n",
        "    gt_energy = np.mean(gt ** 2)\n",
        "    return mse / gt_energy if gt_energy > 0 else float('inf')\n",
        "\n",
        "# Test-Time Augmentation (TTA) function for better results\n",
        "def tta_predict(model, img):\n",
        "    pred = model(img)\n",
        "    pred_hflip = model(torch.flip(img, [3]))  # Horizontal flip\n",
        "    pred_vflip = model(torch.flip(img, [2]))  # Vertical flip\n",
        "    return (pred + torch.flip(pred_hflip, [3]) + torch.flip(pred_vflip, [2])) / 3\n",
        "\n",
        "# Updated evaluate_model function\n",
        "def evaluate_model(model, test_loader, device, use_tta=False):\n",
        "    model.eval()\n",
        "    psnr_scores = []\n",
        "    ssim_scores = []\n",
        "    mae_scores = []\n",
        "    nmse_scores = []\n",
        "    baseline_psnr = []\n",
        "    baseline_ssim = []\n",
        "    baseline_mae = []\n",
        "    baseline_nmse = []\n",
        "    results = []\n",
        "\n",
        "    output_dir = \"/content/Restormer_Project/outputs/derained\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (rain_img, norain_img, filename_tuple) in enumerate(test_loader):\n",
        "            rain_img, norain_img = rain_img.to(device), norain_img.to(device)\n",
        "            filename = filename_tuple[0] # Extract filename string from tuple\n",
        "\n",
        "            # Generate derained image (with optional TTA)\n",
        "            if use_tta:\n",
        "                output_img = tta_predict(model, rain_img)\n",
        "            else:\n",
        "                output_img = model(rain_img)\n",
        "\n",
        "            # Convert to numpy\n",
        "            output_np = output_img.cpu().squeeze().permute(1, 2, 0).clamp(0, 1).numpy()\n",
        "            norain_np = norain_img.cpu().squeeze().permute(1, 2, 0).numpy()\n",
        "            rain_np = rain_img.cpu().squeeze().permute(1, 2, 0).numpy()\n",
        "\n",
        "            # Compute metrics (fix SSIM with win_size=3 and channel_axis=2)\n",
        "            psnr_score = psnr(norain_np, output_np, data_range=1.0)\n",
        "            ssim_score = ssim(norain_np, output_np, channel_axis=2, data_range=1.0, win_size=3)  # Fix here\n",
        "            mae_score = calculate_mae(output_np, norain_np)\n",
        "            nmse_score = calculate_nmse(output_np, norain_np)\n",
        "\n",
        "            psnr_scores.append(psnr_score)\n",
        "            ssim_scores.append(ssim_score)\n",
        "            mae_scores.append(mae_score)\n",
        "            nmse_scores.append(nmse_score)\n",
        "\n",
        "            # Baseline metrics (rainy vs clean)\n",
        "            base_psnr = psnr(norain_np, rain_np, data_range=1.0)\n",
        "            base_ssim = ssim(norain_np, rain_np, channel_axis=2, data_range=1.0, win_size=3)\n",
        "            base_mae = calculate_mae(rain_np, norain_np)\n",
        "            base_nmse = calculate_nmse(rain_np, norain_np)\n",
        "\n",
        "            baseline_psnr.append(base_psnr)\n",
        "            baseline_ssim.append(base_ssim)\n",
        "            baseline_mae.append(base_mae)\n",
        "            baseline_nmse.append(base_nmse)\n",
        "\n",
        "            results.append({\n",
        "                \"Image\": filename,\n",
        "                \"PSNR\": psnr_score,\n",
        "                \"SSIM\": ssim_score,\n",
        "                \"MAE\": mae_score,\n",
        "                \"NMSE\": nmse_score\n",
        "            })\n",
        "\n",
        "            # Save derained image\n",
        "            output_path = os.path.join(output_dir, f\"derained_{filename}\")\n",
        "            plt.imsave(output_path, output_np)\n",
        "\n",
        "    avg_psnr = np.mean(psnr_scores)\n",
        "    avg_ssim = np.mean(ssim_scores)\n",
        "    avg_mae = np.mean(mae_scores)\n",
        "    avg_nmse = np.mean(nmse_scores)\n",
        "    avg_base_psnr = np.mean(baseline_psnr)\n",
        "    avg_base_ssim = np.mean(baseline_ssim)\n",
        "    avg_base_mae = np.mean(baseline_mae)\n",
        "    avg_base_nmse = np.mean(baseline_nmse)\n",
        "\n",
        "    print(f\"Baseline PSNR (rainy vs clean): {avg_base_psnr:.2f} dB\")\n",
        "    print(f\"Baseline SSIM (rainy vs clean): {avg_base_ssim:.4f}\")\n",
        "    print(f\"Baseline MAE (rainy vs clean): {avg_base_mae:.4f}\")\n",
        "    print(f\"Baseline NMSE (rainy vs clean): {avg_base_nmse:.4f}\")\n",
        "\n",
        "    return results, avg_psnr, avg_ssim, avg_mae, avg_nmse\n",
        "\n",
        "# Usage in notebook (after loading model and test_loader)\n",
        "results, avg_psnr, avg_ssim, avg_mae, avg_nmse = evaluate_model(model, test_loader, device, use_tta=True)  # Enable TTA for better results\n",
        "print(f\"Average PSNR: {avg_psnr:.2f} dB\")\n",
        "print(f\"Average SSIM: {avg_ssim:.4f}\")\n",
        "print(f\"Average MAE: {avg_mae:.4f}\")\n",
        "print(f\"Average NMSE: {avg_nmse:.4f}\")\n",
        "\n",
        "# Save extended results to CSV\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(\"/content/Restormer_Project/outputs/evaluation_results_extended.csv\", index=False)"
      ],
      "metadata": {
        "id": "H4E1l70IgUnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import torch\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Metric functions\n",
        "def calculate_mae(pred, gt):\n",
        "    return np.mean(np.abs(pred - gt))\n",
        "\n",
        "def calculate_nmse(pred, gt):\n",
        "    mse = np.mean((pred - gt) ** 2)\n",
        "    gt_energy = np.mean(gt ** 2)\n",
        "    return mse / gt_energy if gt_energy > 0 else float('inf')\n",
        "\n",
        "# Test-Time Augmentation\n",
        "def tta_predict(model, img):\n",
        "    pred = model(img)\n",
        "    pred_hflip = model(torch.flip(img, [3]))  # Horizontal flip\n",
        "    pred_vflip = model(torch.flip(img, [2]))  # Vertical flip\n",
        "    return (pred + torch.flip(pred_hflip, [3]) + torch.flip(pred_vflip, [2])) / 3\n",
        "\n",
        "# Updated evaluate_model\n",
        "def evaluate_model(model, test_loader, device, use_tta=False):\n",
        "    model.eval()\n",
        "    psnr_scores = []\n",
        "    ssim_scores = []\n",
        "    mae_scores = []\n",
        "    nmse_scores = []\n",
        "    baseline_psnr = []\n",
        "    baseline_ssim = []\n",
        "    baseline_mae = []\n",
        "    baseline_nmse = []\n",
        "    results = []\n",
        "\n",
        "    output_dir = \"/content/Restormer_Project/outputs/derained\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (rain_img, norain_img, filename) in enumerate(test_loader):\n",
        "            rain_img, norain_img = rain_img.to(device), norain_img.to(device)\n",
        "\n",
        "            # Ensure normalization (images should be [0,1])\n",
        "            rain_img = rain_img.clamp(0, 1)\n",
        "            norain_img = norain_img.clamp(0, 1)\n",
        "\n",
        "            # Generate derained image\n",
        "            output_img = tta_predict(model, rain_img) if use_tta else model(rain_img)\n",
        "            output_img = output_img.clamp(0, 1)\n",
        "\n",
        "            # Convert to numpy\n",
        "            output_np = output_img.cpu().squeeze().permute(1, 2, 0).numpy()\n",
        "            norain_np = norain_img.cpu().squeeze().permute(1, 2, 0).numpy()\n",
        "            rain_np = rain_img.cpu().squeeze().permute(1, 2, 0).numpy()\n",
        "\n",
        "            # Compute metrics\n",
        "            try:\n",
        "                psnr_score = psnr(norain_np, output_np, data_range=1.0)\n",
        "                ssim_score = ssim(norain_np, output_np, channel_axis=2, data_range=1.0, win_size=3)\n",
        "                mae_score = calculate_mae(output_np, norain_np)\n",
        "                nmse_score = calculate_nmse(output_np, norain_np)\n",
        "\n",
        "                base_psnr = psnr(norain_np, rain_np, data_range=1.0)\n",
        "                base_ssim = ssim(norain_np, rain_np, channel_axis=2, data_range=1.0, win_size=3)\n",
        "                base_mae = calculate_mae(rain_np, norain_np)\n",
        "                base_nmse = calculate_nmse(rain_np, norain_np)\n",
        "            except Exception as e:\n",
        "                print(f\"Error computing metrics for {filename}: {e}\")\n",
        "                continue\n",
        "\n",
        "            psnr_scores.append(psnr_score)\n",
        "            ssim_scores.append(ssim_score)\n",
        "            mae_scores.append(mae_score)\n",
        "            nmse_scores.append(nmse_score)\n",
        "            baseline_psnr.append(base_psnr)\n",
        "            baseline_ssim.append(base_ssim)\n",
        "            baseline_mae.append(base_mae)\n",
        "            baseline_nmse.append(base_nmse)\n",
        "\n",
        "            results.append({\n",
        "                \"Image\": filename[0],  # Handle single-item batch\n",
        "                \"PSNR\": psnr_score,\n",
        "                \"SSIM\": ssim_score,\n",
        "                \"MAE\": mae_score,\n",
        "                \"NMSE\": nmse_score,\n",
        "                \"Baseline_PSNR\": base_psnr,\n",
        "                \"Baseline_SSIM\": base_ssim,\n",
        "                \"Baseline_MAE\": base_mae,\n",
        "                \"Baseline_NMSE\": base_nmse\n",
        "            })\n",
        "\n",
        "            # Save derained image\n",
        "            output_path = os.path.join(output_dir, f\"derained_{filename[0]}\")\n",
        "            plt.imsave(output_path, output_np)\n",
        "\n",
        "    # Compute averages\n",
        "    avg_psnr = np.mean(psnr_scores) if psnr_scores else float('nan')\n",
        "    avg_ssim = np.mean(ssim_scores) if ssim_scores else float('nan')\n",
        "    avg_mae = np.mean(mae_scores) if mae_scores else float('nan')\n",
        "    avg_nmse = np.mean(nmse_scores) if nmse_scores else float('nan')\n",
        "    avg_base_psnr = np.mean(baseline_psnr) if baseline_psnr else float('nan')\n",
        "    avg_base_ssim = np.mean(baseline_ssim) if baseline_ssim else float('nan')\n",
        "    avg_base_mae = np.mean(baseline_mae) if baseline_mae else float('nan')\n",
        "    avg_base_nmse = np.mean(baseline_nmse) if baseline_nmse else float('nan')\n",
        "\n",
        "    print(f\"Baseline PSNR (rainy vs clean): {avg_base_psnr:.2f} dB\")\n",
        "    print(f\"Baseline SSIM (rainy vs clean): {avg_base_ssim:.4f}\")\n",
        "    print(f\"Baseline MAE (rainy vs clean): {avg_base_mae:.4f}\")\n",
        "    print(f\"Baseline NMSE (rainy vs clean): {avg_base_nmse:.4f}\")\n",
        "    print(f\"Average PSNR: {avg_psnr:.2f} dB\")\n",
        "    print(f\"Average SSIM: {avg_ssim:.4f}\")\n",
        "    print(f\"Average MAE: {avg_mae:.4f}\")\n",
        "    print(f\"Average NMSE: {avg_nmse:.4f}\")\n",
        "\n",
        "    return results, avg_psnr, avg_ssim, avg_mae, avg_nmse\n",
        "\n",
        "# Usage (replace in notebook)\n",
        "results, avg_psnr, avg_ssim, avg_mae, avg_nmse = evaluate_model(model, test_loader, device, use_tta=True)\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(\"/content/Restormer_Project/outputs/evaluation_results_extended.csv\", index=False)\n",
        "print(\"Results saved to /content/Restormer_Project/outputs/evaluation_results_extended.csv\")"
      ],
      "metadata": {
        "id": "QXN59w3mif2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_results(model, test_loader, device, num_samples=10, use_tta=False):\n",
        "    model.eval()\n",
        "    samples = list(iter(test_loader))[:num_samples]\n",
        "\n",
        "    plt.figure(figsize=(15, 5 * num_samples))\n",
        "    with torch.no_grad():\n",
        "        for i, (rain_img, norain_img, filename) in enumerate(samples):\n",
        "            rain_img, norain_img = rain_img.to(device), norain_img.to(device)\n",
        "            output_img = tta_predict(model, rain_img) if use_tta else model(rain_img)\n",
        "            output_img = output_img.clamp(0, 1)\n",
        "\n",
        "            output_np = output_img.cpu().squeeze().permute(1, 2, 0).numpy()\n",
        "            norain_np = norain_img.cpu().squeeze().permute(1, 2, 0).numpy()\n",
        "            rain_np = rain_img.cpu().squeeze().permute(1, 2, 0).numpy()\n",
        "\n",
        "            psnr_score = psnr(norain_np, output_np, data_range=1.0)\n",
        "            ssim_score = ssim(norain_np, output_np, channel_axis=2, data_range=1.0, win_size=3)\n",
        "            mae_score = calculate_mae(output_np, norain_np)\n",
        "            nmse_score = calculate_nmse(output_np, norain_np)\n",
        "\n",
        "            plt.subplot(num_samples, 3, i*3 + 1)\n",
        "            plt.title(f\"Rainy: {filename[0]}\")\n",
        "            plt.imshow(rain_np)\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.subplot(num_samples, 3, i*3 + 2)\n",
        "            plt.title(\"Ground Truth\")\n",
        "            plt.imshow(norain_np)\n",
        "            plt.axis('off')\n",
        "\n",
        "            plt.subplot(num_samples, 3, i*3 + 3)\n",
        "            plt.title(f\"Derained\\nPSNR: {psnr_score:.2f}, SSIM: {ssim_score:.4f}\\nMAE: {mae_score:.4f}, NMSE: {nmse_score:.4f}\")\n",
        "            plt.imshow(output_np)\n",
        "            plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"/content/Restormer_Project/outputs/sample_visualizations.png\")\n",
        "    plt.show()\n",
        "\n",
        "# Usage\n",
        "visualize_results(model, test_loader, device, num_samples=10, use_tta=True)"
      ],
      "metadata": {
        "id": "FG_Wxbnmikce"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}